{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   7.839876  163.8499   1095.9736   2814.2742   2863.311    1176.6823\n",
      "  169.69638 ]\n",
      "[    0.            0.            0.           -7.1432953 -2019.7014\n",
      " -2883.5713      -40.315796 ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(DeviceArray(-0.000101, dtype=float32), DeviceArray(-9901.463, dtype=float32))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import matplotlib.pyplot as plt\n",
    "import relaxed\n",
    "import functools\n",
    "from fast_soft_sort.jax_ops import soft_sort\n",
    "\n",
    "np.random.seed(0)\n",
    "nBg = 8000\n",
    "nSig = 300\n",
    "background = np.random.normal(40, 10, nBg)\n",
    "signal = np.random.normal(50, 5, nSig)\n",
    "\n",
    "\n",
    "def significance(S, B):\n",
    "    \"\"\"calculate the significance according to the formula above\"\"\"\n",
    "    return jnp.sqrt(2 * ((S + B) * jnp.log(1 + S / B) - S))\n",
    "\n",
    "\n",
    "def pipeline(pars, data):\n",
    "    s, b = data\n",
    "    bins = pars\n",
    "    sig_hist = relaxed.hist(s, bins=bins, bandwidth=1e-1)\n",
    "    bg_hist = relaxed.hist(b, bins=bins, bandwidth=1e-1)\n",
    "    sig = significance(sig_hist, bg_hist)\n",
    "    return 1 / jnp.nanmean(sig), sig\n",
    "\n",
    "\n",
    "def new_sig(s, b):\n",
    "    n = s+b\n",
    "    print(n)\n",
    "    mu_hat = jnp.mean(n)**.5\n",
    "    print(n*(jnp.log((mu_hat*s + b)/b))- mu_hat*s)\n",
    "    q0 = 2*jnp.sum(n*(jnp.log((mu_hat*s + b)/b)) - mu_hat*s)\n",
    "    return q0#**0.5\n",
    "\n",
    "def pipeline2(pars, data):\n",
    "    s, b = data\n",
    "    bins = pars\n",
    "    sig_hist = relaxed.hist(s, bins=bins, bandwidth=1e-1)\n",
    "    bg_hist = relaxed.hist(b, bins=bins, bandwidth=1e-1)\n",
    "    sig = new_sig(sig_hist, bg_hist)\n",
    "    return 1 / sig, sig\n",
    "\n",
    "\n",
    "pipe = functools.partial(pipeline2, data=(signal, background))\n",
    "init = jnp.linspace(0, 70, 8)\n",
    "pipe(init)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traced<ShapedArray(float32[7])>with<JVPTrace(level=2/1)> with\n",
      "  primal = Traced<ShapedArray(float32[7])>with<DynamicJaxprTrace(level=0/1)>\n",
      "  tangent = Traced<ShapedArray(float32[7])>with<JaxprTrace(level=1/1)> with\n",
      "    pval = (ShapedArray(float32[7]), *)\n",
      "    recipe = JaxprEqnRecipe(eqn_id=<object object at 0x7fc911d4fd40>, invars=(Traced<ShapedArray(float32[7]):JaxprTrace(level=1/1)>, Traced<ShapedArray(float32[7]):JaxprTrace(level=1/1)>), outvars=[<weakref at 0x7fc910a38e00; to 'JaxprTracer' at 0x7fc910a38ef0>], primitive=xla_call, params={'device': None, 'backend': None, 'name': 'jvp(fn)', 'donated_invars': (False, False), 'inline': True, 'call_jaxpr': { lambda ; a:f32[7] b:f32[7]. let c:f32[7] = add a b in (c,) }}, source_info=SourceInfo(traceback=<jaxlib.xla_extension.Traceback object at 0x7fc910a19770>, name_stack=NameStack(stack=(Transform(name='jvp'),))))\n",
      "Traced<ShapedArray(float32[7])>with<JVPTrace(level=2/1)> with\n",
      "  primal = Traced<ShapedArray(float32[7])>with<DynamicJaxprTrace(level=0/1)>\n",
      "  tangent = Traced<ShapedArray(float32[7])>with<JaxprTrace(level=1/1)> with\n",
      "    pval = (ShapedArray(float32[7]), *)\n",
      "    recipe = JaxprEqnRecipe(eqn_id=<object object at 0x7fc911d4ff80>, invars=(Traced<ShapedArray(float32[7]):JaxprTrace(level=1/1)>, Traced<ShapedArray(float32[7]):JaxprTrace(level=1/1)>), outvars=[<weakref at 0x7fc910a62180; to 'JaxprTracer' at 0x7fc910a62360>], primitive=xla_call, params={'device': None, 'backend': None, 'name': 'jvp(<lambda>)', 'donated_invars': (False, False), 'inline': True, 'call_jaxpr': { lambda ; a:f32[7] b:f32[7]. let c:f32[7] = sub a b in (c,) }}, source_info=SourceInfo(traceback=<jaxlib.xla_extension.Traceback object at 0x7fc910a4f8b0>, name_stack=NameStack(stack=(Transform(name='jvp'),))))\n",
      "step 0, loss: -9901.4638671875\n",
      "step 1, loss: -9899.13671875\n",
      "step 2, loss: -9896.7373046875\n",
      "step 3, loss: -9894.2890625\n",
      "step 4, loss: -9891.7958984375\n",
      "step 5, loss: -9889.2734375\n",
      "step 6, loss: -9886.728515625\n",
      "step 7, loss: -9884.1640625\n",
      "step 8, loss: -9881.58984375\n",
      "step 9, loss: -9879.001953125\n",
      "step 10, loss: -9876.3994140625\n",
      "step 11, loss: -9873.77734375\n",
      "step 12, loss: -9871.146484375\n",
      "step 13, loss: -9868.505859375\n",
      "step 14, loss: -9865.8662109375\n",
      "step 15, loss: -9863.23828125\n",
      "step 16, loss: -9860.6240234375\n",
      "step 17, loss: -9858.041015625\n",
      "step 18, loss: -9855.486328125\n",
      "step 19, loss: -9852.9677734375\n",
      "step 20, loss: -9850.484375\n",
      "step 21, loss: -9848.0400390625\n",
      "step 22, loss: -9845.6357421875\n",
      "step 23, loss: -9843.2705078125\n",
      "step 24, loss: -9840.9443359375\n",
      "step 25, loss: -9838.666015625\n",
      "step 26, loss: -9836.44140625\n",
      "step 27, loss: -9834.279296875\n",
      "step 28, loss: -9832.185546875\n",
      "step 29, loss: -9830.177734375\n",
      "step 30, loss: -9828.275390625\n",
      "step 31, loss: -9826.48828125\n",
      "step 32, loss: -9824.8291015625\n",
      "step 33, loss: -9823.3154296875\n",
      "step 34, loss: -9821.9560546875\n",
      "step 35, loss: -9820.7568359375\n",
      "step 36, loss: -9819.71875\n",
      "step 37, loss: -9818.83984375\n",
      "step 38, loss: -9818.1162109375\n",
      "step 39, loss: -9817.537109375\n",
      "step 40, loss: -9817.0859375\n",
      "step 41, loss: -9816.7490234375\n",
      "step 42, loss: -9816.51171875\n",
      "step 43, loss: -9816.3544921875\n",
      "step 44, loss: -9816.259765625\n",
      "step 45, loss: -9816.2216796875\n",
      "step 46, loss: -9816.216796875\n",
      "step 47, loss: -9816.2373046875\n",
      "step 48, loss: -9816.2763671875\n",
      "step 49, loss: -9816.3232421875\n",
      "step 50, loss: -9816.3720703125\n",
      "step 51, loss: -9816.416015625\n",
      "step 52, loss: -9816.45703125\n",
      "step 53, loss: -9816.4853515625\n",
      "step 54, loss: -9816.5029296875\n",
      "step 55, loss: -9816.505859375\n",
      "step 56, loss: -9816.498046875\n",
      "step 57, loss: -9816.474609375\n",
      "step 58, loss: -9816.44140625\n",
      "step 59, loss: -9816.3916015625\n",
      "step 60, loss: -9816.33203125\n",
      "step 61, loss: -9816.2626953125\n",
      "step 62, loss: -9816.18359375\n",
      "step 63, loss: -9816.095703125\n",
      "step 64, loss: -9816.001953125\n",
      "step 65, loss: -9815.9033203125\n",
      "step 66, loss: -9815.798828125\n",
      "step 67, loss: -9815.6943359375\n",
      "step 68, loss: -9815.5869140625\n",
      "step 69, loss: -9815.478515625\n",
      "step 70, loss: -9815.375\n",
      "step 71, loss: -9815.271484375\n",
      "step 72, loss: -9815.173828125\n",
      "step 73, loss: -9815.078125\n",
      "step 74, loss: -9814.9873046875\n",
      "step 75, loss: -9814.9033203125\n",
      "step 76, loss: -9814.8232421875\n",
      "step 77, loss: -9814.7509765625\n",
      "step 78, loss: -9814.6796875\n",
      "step 79, loss: -9814.6142578125\n",
      "step 80, loss: -9814.552734375\n",
      "step 81, loss: -9814.4931640625\n",
      "step 82, loss: -9814.4345703125\n",
      "step 83, loss: -9814.3837890625\n",
      "step 84, loss: -9814.330078125\n",
      "step 85, loss: -9814.279296875\n",
      "step 86, loss: -9814.2294921875\n",
      "step 87, loss: -9814.1796875\n",
      "step 88, loss: -9814.134765625\n",
      "step 89, loss: -9814.0888671875\n",
      "step 90, loss: -9814.0458984375\n",
      "step 91, loss: -9814.001953125\n",
      "step 92, loss: -9813.958984375\n",
      "step 93, loss: -9813.919921875\n",
      "step 94, loss: -9813.880859375\n",
      "step 95, loss: -9813.8447265625\n",
      "step 96, loss: -9813.8125\n",
      "step 97, loss: -9813.7802734375\n",
      "step 98, loss: -9813.751953125\n",
      "step 99, loss: -9813.728515625\n",
      "step 100, loss: -9813.703125\n",
      "step 101, loss: -9813.6796875\n",
      "step 102, loss: -9813.662109375\n",
      "step 103, loss: -9813.646484375\n",
      "step 104, loss: -9813.62890625\n",
      "step 105, loss: -9813.615234375\n",
      "step 106, loss: -9813.6005859375\n",
      "step 107, loss: -9813.5888671875\n",
      "step 108, loss: -9813.578125\n",
      "step 109, loss: -9813.5712890625\n",
      "step 110, loss: -9813.5625\n",
      "step 111, loss: -9813.552734375\n",
      "step 112, loss: -9813.5458984375\n",
      "step 113, loss: -9813.541015625\n",
      "step 114, loss: -9813.5341796875\n",
      "step 115, loss: -9813.5283203125\n",
      "step 116, loss: -9813.525390625\n",
      "step 117, loss: -9813.5166015625\n",
      "step 118, loss: -9813.513671875\n",
      "step 119, loss: -9813.509765625\n",
      "step 120, loss: -9813.505859375\n",
      "step 121, loss: -9813.501953125\n",
      "step 122, loss: -9813.4970703125\n",
      "step 123, loss: -9813.4931640625\n",
      "step 124, loss: -9813.4892578125\n",
      "step 125, loss: -9813.4853515625\n",
      "step 126, loss: -9813.482421875\n",
      "step 127, loss: -9813.4794921875\n",
      "step 128, loss: -9813.47265625\n",
      "step 129, loss: -9813.470703125\n",
      "step 130, loss: -9813.466796875\n",
      "step 131, loss: -9813.46484375\n",
      "step 132, loss: -9813.4599609375\n",
      "step 133, loss: -9813.45703125\n",
      "step 134, loss: -9813.453125\n",
      "step 135, loss: -9813.4482421875\n",
      "step 136, loss: -9813.4462890625\n",
      "step 137, loss: -9813.44140625\n",
      "step 138, loss: -9813.439453125\n",
      "step 139, loss: -9813.4375\n",
      "step 140, loss: -9813.43359375\n",
      "step 141, loss: -9813.431640625\n",
      "step 142, loss: -9813.42578125\n",
      "step 143, loss: -9813.423828125\n",
      "step 144, loss: -9813.419921875\n",
      "step 145, loss: -9813.41796875\n",
      "step 146, loss: -9813.4140625\n",
      "step 147, loss: -9813.4130859375\n",
      "step 148, loss: -9813.41015625\n",
      "step 149, loss: -9813.40625\n",
      "step 150, loss: -9813.404296875\n",
      "step 151, loss: -9813.40234375\n",
      "step 152, loss: -9813.3984375\n",
      "step 153, loss: -9813.396484375\n",
      "step 154, loss: -9813.39453125\n",
      "step 155, loss: -9813.3896484375\n",
      "step 156, loss: -9813.388671875\n",
      "step 157, loss: -9813.3837890625\n",
      "step 158, loss: -9813.3818359375\n",
      "step 159, loss: -9813.3818359375\n",
      "step 160, loss: -9813.3779296875\n",
      "step 161, loss: -9813.375\n",
      "step 162, loss: -9813.373046875\n",
      "step 163, loss: -9813.369140625\n",
      "step 164, loss: -9813.3671875\n",
      "step 165, loss: -9813.365234375\n",
      "step 166, loss: -9813.3623046875\n",
      "step 167, loss: -9813.3603515625\n",
      "step 168, loss: -9813.3564453125\n",
      "step 169, loss: -9813.35546875\n",
      "step 170, loss: -9813.3525390625\n",
      "step 171, loss: -9813.349609375\n",
      "step 172, loss: -9813.34765625\n",
      "step 173, loss: -9813.345703125\n",
      "step 174, loss: -9813.34375\n",
      "step 175, loss: -9813.3408203125\n",
      "step 176, loss: -9813.3369140625\n",
      "step 177, loss: -9813.3359375\n",
      "step 178, loss: -9813.333984375\n",
      "step 179, loss: -9813.3310546875\n",
      "step 180, loss: -9813.330078125\n",
      "step 181, loss: -9813.3271484375\n",
      "step 182, loss: -9813.3251953125\n",
      "step 183, loss: -9813.3193359375\n",
      "step 184, loss: -9813.318359375\n",
      "step 185, loss: -9813.3173828125\n",
      "step 186, loss: -9813.314453125\n",
      "step 187, loss: -9813.3115234375\n",
      "step 188, loss: -9813.3056640625\n",
      "step 189, loss: -9813.3056640625\n",
      "step 190, loss: -9813.30078125\n",
      "step 191, loss: -9813.298828125\n",
      "step 192, loss: -9813.2958984375\n",
      "step 193, loss: -9813.29296875\n",
      "step 194, loss: -9813.2890625\n",
      "step 195, loss: -9813.2890625\n",
      "step 196, loss: -9813.28515625\n",
      "step 197, loss: -9813.2802734375\n",
      "step 198, loss: -9813.27734375\n",
      "step 199, loss: -9813.2744140625\n"
     ]
    }
   ],
   "source": [
    "import optax\n",
    "import celluloid\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rc(\"figure\", figsize=(10, 10), dpi=100, facecolor=\"white\")\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax_copy = ax\n",
    "camera = celluloid.Camera(fig)\n",
    "\n",
    "def fit(params: optax.Params, optimizer: optax.GradientTransformation) -> optax.Params:\n",
    "    opt_state = optimizer.init(params)\n",
    "\n",
    "    @jax.jit\n",
    "    def step(params, opt_state):\n",
    "        grads, loss_value = jax.grad(pipe, has_aux=True)(params)\n",
    "        updates, opt_state = optimizer.update(jnp.nan_to_num(grads), opt_state, params)\n",
    "        params = optax.apply_updates(params, updates)\n",
    "        return params, opt_state, loss_value\n",
    "\n",
    "    for i in range(200):\n",
    "        params, opt_state, loss_value = step(params, opt_state)\n",
    "        ax.hist([background, signal], bins=params, stacked=True, label=[\"background B\", \"signal S\"], color=[\"C0\", \"C1\"])\n",
    "        ax.set_title(f\"iteration {i}, significances: {loss_value}\")\n",
    "        camera.snap()\n",
    "        print(f\"step {i}, loss: {loss_value}\")\n",
    "\n",
    "    return params\n",
    "\n",
    "\n",
    "# Finally, we can fit our parametrized function using the Adam optimizer\n",
    "# provided by optax.\n",
    "optimizer = optax.adam(learning_rate=1e-2)\n",
    "params = fit(init, optimizer)\n",
    "\n",
    "# animate\n",
    "animation = camera.animate()\n",
    "animation.save(\"animation2.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(0.3364722, dtype=float32, weak_type=True)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.log(1.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7cb25e1cc194a96437e8e4e9b40444293fd1e29ec071a9ba5a9ba6b270a7f89c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
