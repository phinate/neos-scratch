{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   7.839876  163.8499   1095.9736   2814.2742   2863.311    1176.6823\n",
      "  169.69638 ]\n",
      "[0.0000000e+00 0.0000000e+00 0.0000000e+00 6.7048073e-03 3.8356781e+00\n",
      " 9.5814362e+00 5.6725502e-02]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(DeviceArray(0.19258893, dtype=float32), DeviceArray(5.1924067, dtype=float32))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import matplotlib.pyplot as plt\n",
    "import relaxed\n",
    "import functools\n",
    "from fast_soft_sort.jax_ops import soft_sort\n",
    "\n",
    "np.random.seed(0)\n",
    "nBg = 8000\n",
    "nSig = 300\n",
    "background = np.random.normal(40, 10, nBg)\n",
    "signal = np.random.normal(50, 5, nSig)\n",
    "\n",
    "\n",
    "def significance(S, B):\n",
    "    \"\"\"calculate the significance according to the formula above\"\"\"\n",
    "    return jnp.sqrt(2 * ((S + B) * jnp.log(1 + S / B) - S))\n",
    "\n",
    "\n",
    "def pipeline(pars, data):\n",
    "    s, b = data\n",
    "    bins = pars\n",
    "    sig_hist = relaxed.hist(s, bins=bins, bandwidth=1e-1)\n",
    "    bg_hist = relaxed.hist(b, bins=bins, bandwidth=1e-1)\n",
    "    sig = significance(sig_hist, bg_hist)\n",
    "    return 1 / jnp.nanmean(sig), sig\n",
    "\n",
    "\n",
    "def new_sig(s, b):\n",
    "    n = s+b\n",
    "    print(n)\n",
    "    mu_hat = jnp.sum(n-b)/jnp.sum(s)\n",
    "    print(n*(jnp.log((mu_hat*s + b)/b))- mu_hat*s)\n",
    "    q0 = 2*jnp.sum(n*(jnp.log((mu_hat*s + b)/b)) - mu_hat*s)\n",
    "    return q0**0.5\n",
    "\n",
    "def pipeline2(pars, data, bw:\n",
    "    s, b = data\n",
    "    bins = pars\n",
    "    sig_hist = relaxed.hist(s, bins=bins, bandwidth=bw)\n",
    "    bg_hist = relaxed.hist(b, bins=bins, bandwidth=bw)\n",
    "    sig = new_sig(sig_hist, bg_hist)\n",
    "    return 1 / sig, sig\n",
    "\n",
    "\n",
    "pipe = functools.partial(pipeline2, data=(signal, background), bw = 1e-4)\n",
    "init = jnp.linspace(0, 70, 8)\n",
    "pipe(init)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traced<ShapedArray(float32[4])>with<JVPTrace(level=2/1)> with\n",
      "  primal = Traced<ShapedArray(float32[4])>with<DynamicJaxprTrace(level=0/1)>\n",
      "  tangent = Traced<ShapedArray(float32[4])>with<JaxprTrace(level=1/1)> with\n",
      "    pval = (ShapedArray(float32[4]), *)\n",
      "    recipe = JaxprEqnRecipe(eqn_id=<object object at 0x7fc902d7b9b0>, invars=(Traced<ShapedArray(float32[4]):JaxprTrace(level=1/1)>, Traced<ShapedArray(float32[4]):JaxprTrace(level=1/1)>), outvars=[<weakref at 0x7fc902dd63b0; to 'JaxprTracer' at 0x7fc902dd6860>], primitive=xla_call, params={'device': None, 'backend': None, 'name': 'jvp(fn)', 'donated_invars': (False, False), 'inline': True, 'call_jaxpr': { lambda ; a:f32[4] b:f32[4]. let c:f32[4] = add a b in (c,) }}, source_info=SourceInfo(traceback=<jaxlib.xla_extension.Traceback object at 0x7fc902dc50b0>, name_stack=NameStack(stack=(Transform(name='jvp'),))))\n",
      "Traced<ShapedArray(float32[4])>with<JVPTrace(level=2/1)> with\n",
      "  primal = Traced<ShapedArray(float32[4])>with<DynamicJaxprTrace(level=0/1)>\n",
      "  tangent = Traced<ShapedArray(float32[4])>with<JaxprTrace(level=1/1)> with\n",
      "    pval = (ShapedArray(float32[4]), *)\n",
      "    recipe = JaxprEqnRecipe(eqn_id=<object object at 0x7fc902d7bc20>, invars=(Traced<ShapedArray(float32[4]):JaxprTrace(level=1/1)>, Traced<ShapedArray(float32[4]):JaxprTrace(level=1/1)>), outvars=[<weakref at 0x7fc902dfcb30; to 'JaxprTracer' at 0x7fc902dfccc0>], primitive=xla_call, params={'device': None, 'backend': None, 'name': 'jvp(<lambda>)', 'donated_invars': (False, False), 'inline': True, 'call_jaxpr': { lambda ; a:f32[4] b:f32[4]. let c:f32[4] = sub a b in (c,) }}, source_info=SourceInfo(traceback=<jaxlib.xla_extension.Traceback object at 0x7fc902de5d30>, name_stack=NameStack(stack=(Transform(name='jvp'),))))\n",
      "step 0, loss: 4.238070964813232\n",
      "step 1, loss: 4.239372730255127\n",
      "step 2, loss: 4.2405686378479\n",
      "step 3, loss: 4.241940498352051\n",
      "step 4, loss: 4.243269443511963\n",
      "step 5, loss: 4.244540214538574\n",
      "step 6, loss: 4.245817184448242\n",
      "step 7, loss: 4.247185230255127\n",
      "step 8, loss: 4.248621940612793\n",
      "step 9, loss: 4.25013542175293\n",
      "step 10, loss: 4.25160026550293\n",
      "step 11, loss: 4.253100872039795\n",
      "step 12, loss: 4.254678249359131\n",
      "step 13, loss: 4.256275177001953\n",
      "step 14, loss: 4.258027076721191\n",
      "step 15, loss: 4.259735107421875\n",
      "step 16, loss: 4.26155424118042\n",
      "step 17, loss: 4.263549327850342\n",
      "step 18, loss: 4.265626430511475\n",
      "step 19, loss: 4.267639636993408\n",
      "step 20, loss: 4.269907474517822\n",
      "step 21, loss: 4.27221155166626\n",
      "step 22, loss: 4.274656295776367\n",
      "step 23, loss: 4.277292251586914\n",
      "step 24, loss: 4.2798542976379395\n",
      "step 25, loss: 4.2826972007751465\n",
      "step 26, loss: 4.285548686981201\n",
      "step 27, loss: 4.288388729095459\n",
      "step 28, loss: 4.291344165802002\n",
      "step 29, loss: 4.294399738311768\n",
      "step 30, loss: 4.297384262084961\n",
      "step 31, loss: 4.300364017486572\n",
      "step 32, loss: 4.303255081176758\n",
      "step 33, loss: 4.30620813369751\n",
      "step 34, loss: 4.309027194976807\n",
      "step 35, loss: 4.311610698699951\n",
      "step 36, loss: 4.314201354980469\n",
      "step 37, loss: 4.31671667098999\n",
      "step 38, loss: 4.319039344787598\n",
      "step 39, loss: 4.321436882019043\n",
      "step 40, loss: 4.323643684387207\n",
      "step 41, loss: 4.325689792633057\n",
      "step 42, loss: 4.327687740325928\n",
      "step 43, loss: 4.3298726081848145\n",
      "step 44, loss: 4.331812858581543\n",
      "step 45, loss: 4.333935737609863\n",
      "step 46, loss: 4.33597469329834\n",
      "step 47, loss: 4.338225364685059\n",
      "step 48, loss: 4.340539455413818\n",
      "step 49, loss: 4.343050003051758\n",
      "step 50, loss: 4.345761299133301\n",
      "step 51, loss: 4.348642826080322\n",
      "step 52, loss: 4.3518290519714355\n",
      "step 53, loss: 4.355410099029541\n",
      "step 54, loss: 4.359387397766113\n",
      "step 55, loss: 4.363924980163574\n",
      "step 56, loss: 4.368898391723633\n",
      "step 57, loss: 4.374499797821045\n",
      "step 58, loss: 4.380712032318115\n",
      "step 59, loss: 4.387598991394043\n",
      "step 60, loss: 4.395106315612793\n",
      "step 61, loss: 4.403143882751465\n",
      "step 62, loss: 4.411771297454834\n",
      "step 63, loss: 4.420759201049805\n",
      "step 64, loss: 4.429940700531006\n",
      "step 65, loss: 4.439328670501709\n",
      "step 66, loss: 4.448639392852783\n",
      "step 67, loss: 4.457962989807129\n",
      "step 68, loss: 4.467004299163818\n",
      "step 69, loss: 4.475718975067139\n",
      "step 70, loss: 4.4841084480285645\n",
      "step 71, loss: 4.492176532745361\n",
      "step 72, loss: 4.499773979187012\n",
      "step 73, loss: 4.507094860076904\n",
      "step 74, loss: 4.513955593109131\n",
      "step 75, loss: 4.520530700683594\n",
      "step 76, loss: 4.526788711547852\n",
      "step 77, loss: 4.532593727111816\n",
      "step 78, loss: 4.538242816925049\n",
      "step 79, loss: 4.5436296463012695\n",
      "step 80, loss: 4.5487775802612305\n",
      "step 81, loss: 4.553545951843262\n",
      "step 82, loss: 4.558272838592529\n",
      "step 83, loss: 4.562721252441406\n",
      "step 84, loss: 4.567092418670654\n",
      "step 85, loss: 4.571322441101074\n",
      "step 86, loss: 4.5753092765808105\n",
      "step 87, loss: 4.579237461090088\n",
      "step 88, loss: 4.582911968231201\n",
      "step 89, loss: 4.58656120300293\n",
      "step 90, loss: 4.590003967285156\n",
      "step 91, loss: 4.593328475952148\n",
      "step 92, loss: 4.596668243408203\n",
      "step 93, loss: 4.599879264831543\n",
      "step 94, loss: 4.602987766265869\n",
      "step 95, loss: 4.605952262878418\n",
      "step 96, loss: 4.608943462371826\n",
      "step 97, loss: 4.6118483543396\n",
      "step 98, loss: 4.614856243133545\n",
      "step 99, loss: 4.6177544593811035\n",
      "step 100, loss: 4.620649337768555\n",
      "step 101, loss: 4.623592376708984\n",
      "step 102, loss: 4.626530647277832\n",
      "step 103, loss: 4.6296515464782715\n",
      "step 104, loss: 4.632626533508301\n",
      "step 105, loss: 4.635785102844238\n",
      "step 106, loss: 4.639000415802002\n",
      "step 107, loss: 4.642212390899658\n",
      "step 108, loss: 4.645558834075928\n",
      "step 109, loss: 4.648969650268555\n",
      "step 110, loss: 4.652502059936523\n",
      "step 111, loss: 4.656105995178223\n",
      "step 112, loss: 4.659739971160889\n",
      "step 113, loss: 4.663308143615723\n",
      "step 114, loss: 4.667096138000488\n",
      "step 115, loss: 4.670759677886963\n",
      "step 116, loss: 4.674557209014893\n",
      "step 117, loss: 4.6782660484313965\n",
      "step 118, loss: 4.681962966918945\n",
      "step 119, loss: 4.685656547546387\n",
      "step 120, loss: 4.689123630523682\n",
      "step 121, loss: 4.692746162414551\n",
      "step 122, loss: 4.696277618408203\n",
      "step 123, loss: 4.69963264465332\n",
      "step 124, loss: 4.703011512756348\n",
      "step 125, loss: 4.706241130828857\n",
      "step 126, loss: 4.709257125854492\n",
      "step 127, loss: 4.712309837341309\n",
      "step 128, loss: 4.7153000831604\n",
      "step 129, loss: 4.7181925773620605\n",
      "step 130, loss: 4.720847129821777\n",
      "step 131, loss: 4.723500728607178\n",
      "step 132, loss: 4.725976943969727\n",
      "step 133, loss: 4.728409290313721\n",
      "step 134, loss: 4.730660915374756\n",
      "step 135, loss: 4.732743740081787\n",
      "step 136, loss: 4.734770774841309\n",
      "step 137, loss: 4.736522674560547\n",
      "step 138, loss: 4.738259792327881\n",
      "step 139, loss: 4.7398505210876465\n",
      "step 140, loss: 4.741201877593994\n",
      "step 141, loss: 4.74252462387085\n",
      "step 142, loss: 4.743714809417725\n",
      "step 143, loss: 4.744733810424805\n",
      "step 144, loss: 4.74575138092041\n",
      "step 145, loss: 4.746703147888184\n",
      "step 146, loss: 4.747487545013428\n",
      "step 147, loss: 4.7482733726501465\n",
      "step 148, loss: 4.748963356018066\n",
      "step 149, loss: 4.749575138092041\n",
      "step 150, loss: 4.75020694732666\n",
      "step 151, loss: 4.750771522521973\n",
      "step 152, loss: 4.7513933181762695\n",
      "step 153, loss: 4.7519659996032715\n",
      "step 154, loss: 4.752434730529785\n",
      "step 155, loss: 4.752986907958984\n",
      "step 156, loss: 4.75351095199585\n",
      "step 157, loss: 4.753994941711426\n",
      "step 158, loss: 4.754519939422607\n",
      "step 159, loss: 4.755032062530518\n",
      "step 160, loss: 4.755674362182617\n",
      "step 161, loss: 4.756185531616211\n",
      "step 162, loss: 4.756713390350342\n",
      "step 163, loss: 4.757203578948975\n",
      "step 164, loss: 4.757777214050293\n",
      "step 165, loss: 4.758420467376709\n",
      "step 166, loss: 4.758953094482422\n",
      "step 167, loss: 4.759539604187012\n",
      "step 168, loss: 4.760108470916748\n",
      "step 169, loss: 4.760693550109863\n",
      "step 170, loss: 4.761322021484375\n",
      "step 171, loss: 4.761941432952881\n",
      "step 172, loss: 4.762613296508789\n",
      "step 173, loss: 4.763274192810059\n",
      "step 174, loss: 4.763895511627197\n",
      "step 175, loss: 4.7645344734191895\n",
      "step 176, loss: 4.765198707580566\n",
      "step 177, loss: 4.765810966491699\n",
      "step 178, loss: 4.766542434692383\n",
      "step 179, loss: 4.767158508300781\n",
      "step 180, loss: 4.767862319946289\n",
      "step 181, loss: 4.768561363220215\n",
      "step 182, loss: 4.769229888916016\n",
      "step 183, loss: 4.769878387451172\n",
      "step 184, loss: 4.77055549621582\n",
      "step 185, loss: 4.771252155303955\n",
      "step 186, loss: 4.771852970123291\n",
      "step 187, loss: 4.772496223449707\n",
      "step 188, loss: 4.773243427276611\n",
      "step 189, loss: 4.773869514465332\n",
      "step 190, loss: 4.774461269378662\n",
      "step 191, loss: 4.775094985961914\n",
      "step 192, loss: 4.775749683380127\n",
      "step 193, loss: 4.776310443878174\n",
      "step 194, loss: 4.7769293785095215\n",
      "step 195, loss: 4.777555465698242\n",
      "step 196, loss: 4.778156757354736\n",
      "step 197, loss: 4.7787556648254395\n",
      "step 198, loss: 4.7793803215026855\n",
      "step 199, loss: 4.779953479766846\n"
     ]
    }
   ],
   "source": [
    "import optax\n",
    "import celluloid\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rc(\"figure\", figsize=(10, 10), dpi=100, facecolor=\"white\")\n",
    "\n",
    "fig, axs = plt.subplots(1, 2)\n",
    "ax_copy = ax\n",
    "camera = celluloid.Camera(fig)\n",
    "\n",
    "def fit(params: optax.Params, optimizer: optax.GradientTransformation) -> optax.Params:\n",
    "    opt_state = optimizer.init(params)\n",
    "\n",
    "    @jax.jit\n",
    "    def step(params, opt_state):\n",
    "        grads, loss_value = jax.grad(pipe, has_aux=True)(params)\n",
    "        updates, opt_state = optimizer.update(jnp.nan_to_num(grads), opt_state, params)\n",
    "        params = optax.apply_updates(params, updates)\n",
    "        return params, opt_state, loss_value\n",
    "    sigs = []\n",
    "    for i in range(200):\n",
    "        params, opt_state, loss_value = step(params, opt_state)\n",
    "        ax = axs[0]\n",
    "        ax.hist([background, signal], bins=params, stacked=True, label=[\"background B\", \"signal S\"], color=[\"C0\", \"C1\"])\n",
    "        ax.text(s=f\"iteration {i}, significance over all bins: {loss_value:.5f}\", x=0.05, y=0.95, transform=ax.transAxes)\n",
    "        if i == 0:\n",
    "            ax.legend()\n",
    "        # plot the current state of the optimization\n",
    "        ax = axs[1]\n",
    "        sigs.append(loss_value)\n",
    "        ax.plot(sigs, color=\"C9\")\n",
    "        ax.set_xlabel(\"iteration\")\n",
    "        ax.set_ylabel(\"significance\")\n",
    "        plt.tight_layout()\n",
    "        camera.snap()\n",
    "        print(f\"step {i}, loss: {loss_value}\")\n",
    "\n",
    "    return params\n",
    "\n",
    "\n",
    "# Finally, we can fit our parametrized function using the Adam optimizer\n",
    "# provided by optax.\n",
    "optimizer = optax.adam(learning_rate=1e-2)\n",
    "init = jnp.linspace(0, 70, 5)\n",
    "params = fit(init, optimizer)\n",
    "\n",
    "# animate\n",
    "animation = camera.animate()\n",
    "animation.save(\"animation3.gif\", fps=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(0.3364722, dtype=float32, weak_type=True)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.log(1.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7cb25e1cc194a96437e8e4e9b40444293fd1e29ec071a9ba5a9ba6b270a7f89c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
